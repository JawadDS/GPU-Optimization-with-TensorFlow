{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Optimisation\n",
        "\n",
        "The notebook you have been provided contains code for a convolutional neural network and an image pre-processing and augmentation pipeline. The network and data pipeline are poorly optimised. Your task for this assignment is to analyse the performance of the network, in terms of training speed and the quality of the results, and then make improvements to the network based on your findings.\n",
        "\n",
        "You will be training the model on the Oxford-IIIT pet dataset. The pipeline has a preprocessing function that performs some data augmentation. Consider how the pipeline can be made more efficient, use the Tensorboard Profiler tool to help—note you only need to profile a couple of batches, not the full training process.\n",
        "\n",
        "The CNN uses a VGGNet architechture, built up with blocks of convolutional layers and maxpooling layers. Consider the settings of the existing layers, and whether additional layers can be added to improve the model accuracy, speed up learning, and tackle issues like over fitting. Research decisions around activation functions, optimiser settings.\n",
        "\n",
        "Train the model as is at least once, and analyse the results—to speed up the notebook for future runs, you can save the weights and reload them to perform analysis. If you do save trhe weights, be sure to download them or store them in your google drive for easy access later on. The initial model could take up to an hour to run, so plan your time ahead.\n",
        "\n",
        "You should create plots of the loss and metric curves, and to demonstrate the predictive capabilities of the model—think about what types of images the model is getting correct and what it is getting wrong.\n",
        "\n",
        "You might want to add in callbacks to control modify training, but this isn't strictly necessary.\n",
        "\n",
        "(I can't stress enough that this model and pipeline are *bad* and that is deliberate. It should just about learn *something* with no changes, but it won't do a good job. You can change anything in the code below, except for the block that loads the dataset—no training the model on a different set of images. You should be aiming to improve three key aspects in order to achieve full marks—training speed, accuracy of the training dataset, and accuracy of the validation/testing dataset.)"
      ],
      "metadata": {
        "id": "Me0BtBWiFuod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task for this assignment is to analyse the performance of the network, in terms of training speed and the quality of the results, and then make improvements to the network based on your findings.\n",
        "\n",
        "1.\tTrain the network and plot the resulting loss functions and metrics. Create plots that demonstrate the predictive results of the network. Comment on these results in your report. (10 marks)\n",
        "2.\tAddress the performance issues in the image processing pipeline, comment on the choices you make in your report. (10 marks)\n",
        "3.\tAdjust the design of the CNN to achieve improved predictive results, comment on the choices you make in your report. (10 marks)\n",
        "4.\tChange the settings used in training the network to improve both the training speed and predictive results. (5 marks)\n",
        "5.\tTrain the new network, and repeat the analysis performed in the first step. Comment in your report how the changes you made have improved the speed and performance of the network. (10 marks)\n",
        "Along with your completed Notebook, you should produce a 2-page report as detailed above. (5 marks for writing quality)\n"
      ],
      "metadata": {
        "id": "vWJYCCkgbeep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1 (10 marks)\n",
        "\n",
        "Train the network and plot the resulting loss functions and metrics. Create plots that demonstrate the predictive results of the network. Comment on these results in your report. If you are worried about space in your report, you can number your plots inside the notebook and refer to those numbers in the text."
      ],
      "metadata": {
        "id": "pg9xV3ail44p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tensorflow addons package,\n",
        "# which has a nice image rotation function\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "Y1M3Fpb71Ar7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4Eaj7pBSjH-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkU0EFXChMR_"
      },
      "outputs": [],
      "source": [
        "# Define some Important Variables\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "IMG_SIZE = (img_height,img_width)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Preprocessing functions. You should aim to optimise these in Exercise 2.\n",
        "\n",
        "def img_process_train(features):\n",
        "    \"\"\"\n",
        "    A preprocessing fuction for the training dataset. This function accepts the\n",
        "    oxford_iiit_pet dataset, extracts the images and species label, the performs\n",
        "    random augmentations before resizing and rescaling the images.\n",
        "    \"\"\"\n",
        "    image = features['image']\n",
        "    label = features['species']\n",
        "\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, 0.1)\n",
        "    image = tf.image.random_contrast(image, 0.2, 0.5)\n",
        "    image = tf.image.random_hue(image, 0.2)\n",
        "    image = tf.image.random_jpeg_quality(image, 75, 95)\n",
        "    image = tf.image.random_saturation(image, 5, 10)\n",
        "\n",
        "    # Use tensorflow addons to randomly rotate images\n",
        "    deg = np.random.uniform(-20,20)\n",
        "    image = tfa.image.rotate(image, deg)\n",
        "\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, 'float32')/255.\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def img_process_test(features):\n",
        "    \"\"\"\n",
        "    A preprocessing fuction for the test and validation datasets. This function\n",
        "    accepts the oxford_iiit_pet dataset, extracts the images and species label,\n",
        "    and resizes and rescales the images.\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(features['image'], IMG_SIZE)\n",
        "    image = tf.cast(image, 'float32')/255.\n",
        "    return image, features['species']\n"
      ],
      "metadata": {
        "id": "KNL1FcdKlOuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "### DONT CHANGE ME ###\n",
        "train_ds, val_ds, test_ds = tfds.load(\n",
        "\"oxford_iiit_pet\",\n",
        "split=[\"train[:100%]\", \"test[:50%]\", \"test[50%:100%]\"],\n",
        ")\n",
        "\n",
        "### DONT CHANGE ME ###\n",
        "\n",
        "# Set up the datasets with the augmentations and resizing\n",
        "train_ds = train_ds.map(img_process_train).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(img_process_test).batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(img_process_test).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "6X_ZOioLjA2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Model Definition\n",
        "def create_model_unoptimised():\n",
        "    model = Sequential([\n",
        "    layers.Input(shape=IMG_SIZE+(3,), name='Input'),\n",
        "    layers.Conv2D(32, 4, padding='same', activation='relu',\n",
        "                    name='Conv_1'),\n",
        "    layers.Conv2D(32, 4, padding='same', activation='relu',\n",
        "                    name='Conv_2'),\n",
        "    layers.MaxPooling2D(name='Pool_1'),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu',\n",
        "                    name='Conv_3'),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu',\n",
        "                    name='Conv_4'),\n",
        "    layers.MaxPooling2D(name='Pool_2'),\n",
        "    layers.Flatten(name='Flatten'),\n",
        "    layers.Dense(512, activation='relu', name='dense_1'),\n",
        "    layers.Dense(1, name='Output')\n",
        "    ], name='CNN')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MOBuSzxRiusF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a version of the model and print the summary\n",
        "model = create_model_unoptimised()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-NiZ4uoKnJcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimiser\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "# Binary classification loss\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Accuracy metric\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss_obj,\n",
        "              metrics=metrics)\n"
      ],
      "metadata": {
        "id": "k1U1i8YSi5Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=EPOCHS)\n"
      ],
      "metadata": {
        "id": "187JJxGJi6IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2 (10 marks)\n",
        "Address the performance issues in the image processing pipeline, comment on the choices you make in your report.\n",
        "\n",
        "You can copy the code above and make changes, or write the piepline from scratch. This could include creating new preprocessing functions.\n",
        "\n",
        "Use this exercise to change the definitions of the training, validation and testing datasets to improve the speed at which the network trains, ***and*** predictive performance."
      ],
      "metadata": {
        "id": "WlkPaKJgmhqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "### DONT CHANGE ME ###\n",
        "train_ds, val_ds, test_ds = tfds.load(\n",
        "\"oxford_iiit_pet\",\n",
        "split=[\"train[:100%]\", \"test[:50%]\", \"test[50%:100%]\"],\n",
        ")\n",
        "\n",
        "### DONT CHANGE ME ###"
      ],
      "metadata": {
        "id": "edA-qIaR-UaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Edit the code below to improve the training speed and predictive performance\n",
        "### of the network. Consider how to better implement the img_process_train and\n",
        "### img_process test fuctions.\n",
        "\n",
        "# Set up the datasets with the augmentations and resizing\n",
        "train_ds = train_ds.map(img_process_train).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(img_process_test).batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(img_process_test).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "9YPfakuxlq1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3 (10 marks)\n",
        "Adjust the design of the CNN to achieve improved predictive results, comment on the choices you make in your report.\n",
        "\n",
        "You can add in new layers, change the settings of the existing layers. You could even use a different CNN architecture. Consider how the layer settings contribute to slowing down the network, such as with very large calculations with high numbers of traininable parameters.\n",
        "\n",
        "Comment on any changes you make in your report, including details of any tests you performed. You might find it useful to create a new notebook specifically for testing different models, so that this notebook doesn't get too clogged up with outputs."
      ],
      "metadata": {
        "id": "YNlPJAJjnQIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Change the model architecture to improve both the training speed and\n",
        "### predictive performance\n",
        "\n",
        "# Sequential Model Definition\n",
        "def create_model_optimised():\n",
        "    model = Sequential([\n",
        "    layers.Input(shape=IMG_SIZE+(3,), name='Input'),\n",
        "    layers.Conv2D(32, 4, padding='same', activation='relu',\n",
        "                    name='Conv_1'),\n",
        "    layers.Conv2D(32, 4, padding='same', activation='relu',\n",
        "                    name='Conv_2'),\n",
        "    layers.MaxPooling2D(name='Pool_1'),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu',\n",
        "                    name='Conv_3'),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu',\n",
        "                    name='Conv_4'),\n",
        "    layers.MaxPooling2D(name='Pool_2'),\n",
        "    layers.Flatten(name='Flatten'),\n",
        "    layers.Dense(512, activation='relu', name='dense_1'),\n",
        "    layers.Dense(1, name='Output')\n",
        "    ], name='CNN')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "R4EDkyVGoAGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4 (5 marks)\n",
        "\n",
        "Change the settings used in training the network to improve both the training speed and predictive results.\n",
        "\n",
        "Consider how you might compile the model with different settings, how many epochs the model needs to be trained for, how learning rates and batch sizes might affect training."
      ],
      "metadata": {
        "id": "FtHjLdK3oAcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Experiment with the training settings to maximise the predictive performance\n",
        "### of the network\n",
        "\n",
        "# Adam optimiser\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "# Binary classification loss\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Accuracy metric\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss_obj,\n",
        "              metrics=metrics)\n"
      ],
      "metadata": {
        "id": "OiDKVMDboT5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5 (10 marks)\n",
        "\n",
        "Train the new network, and repeat the analysis performed in the first step. Comment in your report how the changes you made have improved the speed and performance of the network."
      ],
      "metadata": {
        "id": "p_9fTMDRoUUB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5z-7zYCodC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}